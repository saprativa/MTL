{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4a4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import LEDForConditionalGeneration, AutoTokenizer\n",
    "import copy\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0e4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"allenai/led-base-16384\"\n",
    "batch_size = 1\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d823f557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1b6e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 18 22:24:27 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      Off  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    42W / 250W |  39975MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-PCIE-40GB      Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    40W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-PCIE-40GB      Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    37W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-PCIE-40GB      Off  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    37W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-PCIE-40GB      Off  | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    38W / 250W |   9187MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     15344      C   python                           1971MiB |\n",
      "|    0   N/A  N/A   2002260      C   .../obj_detection/bin/python    36901MiB |\n",
      "|    0   N/A  N/A   2026001      C   python                            603MiB |\n",
      "|    0   N/A  N/A   2839719      C   python                            497MiB |\n",
      "|    1   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    2   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    3   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A     65424      C   ...onda3/envs/sb3/bin/python     8601MiB |\n",
      "|    4   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9147b647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a13d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "# define the class\n",
    "class MLT(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_checkpoint):\n",
    "      super(MLT, self).__init__()\n",
    "\n",
    "      self.model = LEDForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "      self.encoder = self.model.get_encoder()\n",
    "\n",
    "      self.decoder1 = self.model.get_decoder()\n",
    "      self.decoder2 = copy.deepcopy(self.decoder1)\n",
    "\n",
    "      self.lm_head1 = self.model.get_output_embeddings()\n",
    "      self.lm_head2 = copy.deepcopy(self.lm_head1)\n",
    "\n",
    "    def get_config(self):\n",
    "      return self.model.config\n",
    "\n",
    "    def get_decoder(self):\n",
    "      return self.decoder1\n",
    "\n",
    "    def get_lm_head(self):\n",
    "      return self.lm_head1\n",
    "\n",
    "    def get_final_logits_bias(self):\n",
    "      return self.model.final_logits_bias\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "      return self.tokenizer\n",
    "\n",
    "    def forward(self, text, summary1, summary2):\n",
    "      # inputs = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "      # target1 = self.tokenizer.encode(summary1, return_tensors=\"pt\")\n",
    "      # target2 = self.tokenizer.encode(summary2, return_tensors=\"pt\")\n",
    "\n",
    "      inputs = text\n",
    "      target1 = summary1\n",
    "      target2 = summary2\n",
    "\n",
    "      encoder_outputs = self.encoder(inputs)\n",
    "\n",
    "      decoder_input_ids1 = shift_tokens_right(\n",
    "                    target1, self.model.config.pad_token_id, self.model.config.decoder_start_token_id\n",
    "                )\n",
    "      \n",
    "      decoder_input_ids2 = shift_tokens_right(\n",
    "                    target2, self.model.config.pad_token_id, self.model.config.decoder_start_token_id\n",
    "                )\n",
    "      \n",
    "      \n",
    "      decoder_outputs1 = self.decoder1(\n",
    "          decoder_input_ids1, \n",
    "          encoder_hidden_states=encoder_outputs[0], \n",
    "          use_cache = False,\n",
    "          output_attentions=self.model.config.output_attentions,\n",
    "          output_hidden_states=self.model.config.output_hidden_states,\n",
    "          return_dict=self.model.config.use_return_dict,\n",
    "          ) \n",
    "\n",
    "      decoder_outputs2 = self.decoder2(\n",
    "          decoder_input_ids2, \n",
    "          encoder_hidden_states=encoder_outputs[0], \n",
    "          use_cache = False,\n",
    "          output_attentions=self.model.config.output_attentions,\n",
    "          output_hidden_states=self.model.config.output_hidden_states,\n",
    "          return_dict=self.model.config.use_return_dict,\n",
    "          )  \n",
    "\n",
    "      lm_logits1 = self.lm_head1(decoder_outputs1[0]) + self.model.final_logits_bias\n",
    "      lm_logits2 = self.lm_head2(decoder_outputs2[0]) + self.model.final_logits_bias   \n",
    "\n",
    "      masked_lm_loss1 = None\n",
    "      masked_lm_loss2 = None\n",
    "      loss_fct = CrossEntropyLoss()\n",
    "      masked_lm_loss1 = loss_fct(lm_logits1.view(-1, self.model.config.vocab_size), target1.view(-1))\n",
    "      masked_lm_loss2 = loss_fct(lm_logits2.view(-1, self.model.config.vocab_size), target2.view(-1))\n",
    "      \n",
    "      # return {\n",
    "      #     'loss1': masked_lm_loss1, \n",
    "      #     'loss2': masked_lm_loss2,\n",
    "      #     'encoder_outputs': encoder_outputs\n",
    "      #     }\n",
    "\n",
    "      return (masked_lm_loss1, masked_lm_loss2, encoder_outputs)\n",
    "\n",
    "\n",
    "# create the object\n",
    "model = MLT(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27590c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLT(\n",
       "  (model): LEDForConditionalGeneration(\n",
       "    (led): LEDModel(\n",
       "      (shared): Embedding(50265, 768, padding_idx=1)\n",
       "      (encoder): LEDEncoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): LEDLearnedPositionalEmbedding(16384, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0): LEDEncoderLayer(\n",
       "            (self_attn): LEDEncoderAttention(\n",
       "              (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): LEDEncoderLayer(\n",
       "            (self_attn): LEDEncoderAttention(\n",
       "              (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): LEDEncoderLayer(\n",
       "            (self_attn): LEDEncoderAttention(\n",
       "              (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): LEDEncoderLayer(\n",
       "            (self_attn): LEDEncoderAttention(\n",
       "              (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): LEDEncoderLayer(\n",
       "            (self_attn): LEDEncoderAttention(\n",
       "              (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): LEDEncoderLayer(\n",
       "            (self_attn): LEDEncoderAttention(\n",
       "              (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): LEDDecoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0): LEDDecoderLayer(\n",
       "            (self_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): LEDDecoderLayer(\n",
       "            (self_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): LEDDecoderLayer(\n",
       "            (self_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): LEDDecoderLayer(\n",
       "            (self_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): LEDDecoderLayer(\n",
       "            (self_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): LEDDecoderLayer(\n",
       "            (self_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): LEDDecoderAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       "  (encoder): LEDEncoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): LEDLearnedPositionalEmbedding(16384, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): LEDEncoderLayer(\n",
       "        (self_attn): LEDEncoderAttention(\n",
       "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): LEDEncoderLayer(\n",
       "        (self_attn): LEDEncoderAttention(\n",
       "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): LEDEncoderLayer(\n",
       "        (self_attn): LEDEncoderAttention(\n",
       "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): LEDEncoderLayer(\n",
       "        (self_attn): LEDEncoderAttention(\n",
       "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): LEDEncoderLayer(\n",
       "        (self_attn): LEDEncoderAttention(\n",
       "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): LEDEncoderLayer(\n",
       "        (self_attn): LEDEncoderAttention(\n",
       "          (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder1): LEDDecoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder2): LEDDecoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head1): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  (lm_head2): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc85895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "137\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "transcripts_dir = Path(\"./data/ami/transcripts\")\n",
    "abs_summaries_dir = Path(\"./data/ami/summaries/abstractive\")\n",
    "ext_summaries_dir = Path(\"./data/ami/summaries/extractive\")\n",
    "\n",
    "transcripts = []\n",
    "abs_summaries = []\n",
    "ext_summaries = []\n",
    "\n",
    "for file in transcripts_dir.iterdir():\n",
    "  transcripts.append(file.read_text())\n",
    "\n",
    "for file in abs_summaries_dir.iterdir():\n",
    "  abs_summaries.append(file.read_text())\n",
    "\n",
    "for file in ext_summaries_dir.iterdir():\n",
    "  ext_summaries.append(file.read_text())\n",
    "\n",
    "\n",
    "transcripts = [s[:3000] for s in transcripts]\n",
    "abs_summaries = [s[:3000] for s in abs_summaries]\n",
    "ext_summaries = [s[:3000] for s in ext_summaries]\n",
    "\n",
    "print(len(transcripts))\n",
    "print(len(abs_summaries))\n",
    "print(len(ext_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58b2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_transcripts, val_transcripts, train_abs_summaries, val_abs_summaries = train_test_split(transcripts, abs_summaries, test_size=.2)\n",
    "_, _, train_ext_summaries, val_ext_summaries = train_test_split(transcripts, ext_summaries, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0016b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "train_transcripts_encodings = tokenizer(train_transcripts, truncation=True, padding=True)\n",
    "val_transcripts_encodings = tokenizer(val_transcripts, truncation=True, padding=True)\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "  train_abs_summaries_encodings = tokenizer(train_abs_summaries, truncation=True, padding=True)\n",
    "  val_abs_summaries_encodings = tokenizer(val_abs_summaries, truncation=True, padding=True)\n",
    "\n",
    "  train_ext_summaries_encodings = tokenizer(train_ext_summaries, truncation=True, padding=True)\n",
    "  val_ext_summaries_encodings = tokenizer(val_ext_summaries, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0e2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transcripts, abs_summaries, ext_summaries):\n",
    "        self.transcripts = transcripts\n",
    "        self.abs_summaries = abs_summaries\n",
    "        self.ext_summaries = ext_summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transcripts.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.transcripts.items()}\n",
    "        item[\"abs\"] = torch.tensor(self.abs_summaries[\"input_ids\"][idx])\n",
    "        item[\"ext\"] = torch.tensor(self.ext_summaries[\"input_ids\"][idx])\n",
    "        return item\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = MeetDataset(train_transcripts_encodings, train_abs_summaries_encodings, train_ext_summaries_encodings)\n",
    "val_dataset = MeetDataset(val_transcripts_encodings, val_abs_summaries_encodings, val_ext_summaries_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2175d04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__(), val_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f20ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f49d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LEDPreTrainedModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class DecoderForGeneration(LEDPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, decoder, lm_head, final_logits_bias):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.config = config\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "        self.final_logits_bias = final_logits_bias\n",
    "\n",
    "    # def get_encoder(self):\n",
    "    #     return self.get_encoder()\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.get_decoder()\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
    "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
    "        self._resize_final_logits_bias(new_num_tokens)\n",
    "        return new_embeddings\n",
    "\n",
    "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
    "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
    "        if new_num_tokens <= old_num_tokens:\n",
    "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
    "        else:\n",
    "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
    "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
    "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            # if use_cache:\n",
    "            #     logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
    "            use_cache = False\n",
    "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "\n",
    "        decoder_outputs = self.decoder(\n",
    "        decoder_input_ids, \n",
    "        encoder_hidden_states=encoder_outputs[0], \n",
    "        use_cache = False,\n",
    "        output_attentions=self.config.output_attentions,\n",
    "        output_hidden_states=self.config.output_hidden_states,\n",
    "        return_dict=self.config.use_return_dict,\n",
    "        )\n",
    "        lm_logits = self.lm_head(decoder_outputs[0]) + self.final_logits_bias\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        decoder_input_ids,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"past_key_values\": past,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
    "        }\n",
    "\n",
    "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
    "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
    "            )\n",
    "        return reordered_past\n",
    "\n",
    "\n",
    "myDecoderModel = DecoderForGeneration(model.get_config(), model.get_decoder(), model.get_lm_head(), model.get_final_logits_bias())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec54ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderForGeneration(\n",
       "  (decoder): LEDDecoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): LEDLearnedPositionalEmbedding(1024, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): LEDDecoderLayer(\n",
       "        (self_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): LEDDecoderAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDecoderModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e5dd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4284777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2959250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/tanik_1821cs08/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20.74709701538086\n",
      "46.59455871582031\n",
      "22.667743682861328\n",
      "16.176712036132812\n",
      "11.575379371643066\n",
      "9.77490234375\n",
      "11.350834846496582\n",
      "10.061376571655273\n",
      "15.193272590637207\n",
      "12.038455963134766\n",
      "14.738117218017578\n",
      "11.026823043823242\n",
      "11.56887435913086\n",
      "11.104249954223633\n",
      "12.988412857055664\n",
      "10.315618515014648\n",
      "11.01276969909668\n",
      "10.664752960205078\n",
      "10.3236083984375\n",
      "7.984981060028076\n",
      "10.867842674255371\n",
      "9.989913940429688\n",
      "10.624231338500977\n",
      "10.20520305633545\n",
      "11.027393341064453\n",
      "7.871992588043213\n",
      "7.888255596160889\n",
      "10.033729553222656\n",
      "10.07597827911377\n",
      "9.076176643371582\n",
      "8.36831283569336\n",
      "9.452491760253906\n",
      "9.978582382202148\n",
      "10.215555191040039\n",
      "9.299059867858887\n",
      "10.417692184448242\n",
      "9.393657684326172\n",
      "10.25532341003418\n",
      "8.456676483154297\n",
      "13.433917045593262\n",
      "9.868483543395996\n",
      "11.306121826171875\n",
      "9.959053993225098\n",
      "9.004140853881836\n",
      "8.072429656982422\n",
      "9.92113208770752\n",
      "9.74963092803955\n",
      "9.006385803222656\n",
      "9.217567443847656\n",
      "7.8364481925964355\n",
      "11.253679275512695\n",
      "8.8090181350708\n",
      "9.689208984375\n",
      "9.83572006225586\n",
      "10.694730758666992\n",
      "9.880509376525879\n",
      "10.011001586914062\n",
      "9.758267402648926\n",
      "8.233002662658691\n",
      "9.974193572998047\n",
      "9.765451431274414\n",
      "9.135807037353516\n",
      "9.41617202758789\n",
      "8.427534103393555\n",
      "8.01664924621582\n",
      "8.75490665435791\n",
      "8.76781940460205\n",
      "8.945194244384766\n",
      "9.166901588439941\n",
      "9.202609062194824\n",
      "9.20506763458252\n",
      "15.927753448486328\n",
      "13.421772003173828\n",
      "9.556957244873047\n",
      "9.195497512817383\n",
      "9.135483741760254\n",
      "8.632652282714844\n",
      "8.271915435791016\n",
      "7.483761310577393\n",
      "8.87357234954834\n",
      "8.283016204833984\n",
      "7.934699058532715\n",
      "8.441624641418457\n",
      "9.300378799438477\n",
      "8.302149772644043\n",
      "9.68297290802002\n",
      "9.592891693115234\n",
      "9.314807891845703\n",
      "9.375038146972656\n",
      "8.435550689697266\n",
      "8.793706893920898\n",
      "9.225010871887207\n",
      "8.58942699432373\n",
      "9.272998809814453\n",
      "8.420184135437012\n",
      "8.647761344909668\n",
      "8.005106925964355\n",
      "9.137279510498047\n",
      "9.568439483642578\n",
      "8.32113265991211\n",
      "8.667080879211426\n",
      "8.085783004760742\n",
      "7.500723361968994\n",
      "8.372482299804688\n",
      "8.979283332824707\n",
      "7.901732444763184\n",
      "8.377303123474121\n",
      "9.134471893310547\n",
      "8.743911743164062\n",
      "Validation\n",
      "11.435213088989258\n",
      "13.228023529052734\n",
      "13.661825180053711\n",
      "12.369211196899414\n",
      "11.3057279586792\n",
      "12.397554397583008\n",
      "9.56836986541748\n",
      "10.565723419189453\n",
      "10.750868797302246\n",
      "13.350105285644531\n",
      "10.126461029052734\n",
      "13.840400695800781\n",
      "12.310396194458008\n",
      "12.712060928344727\n",
      "12.041542053222656\n",
      "13.388018608093262\n",
      "12.953502655029297\n",
      "11.375778198242188\n",
      "11.622457504272461\n",
      "10.232547760009766\n",
      "10.925292015075684\n",
      "10.08349895477295\n",
      "11.644624710083008\n",
      "12.859701156616211\n",
      "10.617210388183594\n",
      "13.109100341796875\n",
      "13.068632125854492\n",
      "10.955184936523438\n",
      "1\n",
      "8.860578536987305\n",
      "8.530713081359863\n",
      "8.315574645996094\n",
      "8.811482429504395\n",
      "9.205967903137207\n",
      "8.714730262756348\n",
      "8.240671157836914\n",
      "7.306270599365234\n",
      "9.29394817352295\n",
      "8.604398727416992\n",
      "7.644995212554932\n",
      "8.095364570617676\n",
      "8.702160835266113\n",
      "7.1956562995910645\n",
      "14.663944244384766\n",
      "12.704336166381836\n",
      "8.826377868652344\n",
      "7.932237148284912\n",
      "9.429502487182617\n",
      "7.67819356918335\n",
      "9.524420738220215\n",
      "8.66061782836914\n",
      "8.305031776428223\n",
      "8.206042289733887\n",
      "8.935630798339844\n",
      "9.678884506225586\n",
      "8.930814743041992\n",
      "7.82731819152832\n",
      "9.881595611572266\n",
      "8.848443984985352\n",
      "8.164200782775879\n",
      "8.521465301513672\n",
      "9.035810470581055\n",
      "8.055909156799316\n",
      "9.227775573730469\n",
      "9.120820999145508\n",
      "8.910623550415039\n",
      "9.124781608581543\n",
      "9.346949577331543\n",
      "9.192244529724121\n",
      "8.702875137329102\n",
      "8.658089637756348\n",
      "8.944612503051758\n",
      "9.3482084274292\n",
      "7.888071060180664\n",
      "9.060724258422852\n",
      "7.5614333152771\n",
      "9.297390937805176\n",
      "7.036977767944336\n",
      "9.836633682250977\n",
      "8.541616439819336\n",
      "9.071951866149902\n",
      "8.775629043579102\n",
      "7.9240312576293945\n",
      "8.628093719482422\n",
      "8.851567268371582\n",
      "9.845050811767578\n",
      "7.038236618041992\n",
      "12.908086776733398\n",
      "9.058127403259277\n",
      "9.528311729431152\n",
      "9.482036590576172\n",
      "8.984489440917969\n",
      "9.45159912109375\n",
      "7.377036094665527\n",
      "8.6616792678833\n",
      "9.205497741699219\n",
      "8.103521347045898\n",
      "9.14413070678711\n",
      "8.9970064163208\n",
      "8.886992454528809\n",
      "7.873253345489502\n",
      "9.307694435119629\n",
      "8.65733814239502\n",
      "8.094858169555664\n",
      "7.434967517852783\n",
      "9.055248260498047\n",
      "7.602088928222656\n",
      "8.228267669677734\n",
      "9.355718612670898\n",
      "8.528769493103027\n",
      "8.566307067871094\n",
      "8.532492637634277\n",
      "9.211440086364746\n",
      "7.891080856323242\n",
      "9.12508773803711\n",
      "8.639853477478027\n",
      "9.25441837310791\n",
      "8.731614112854004\n",
      "8.44365119934082\n",
      "7.478669166564941\n",
      "8.263757705688477\n",
      "7.156060695648193\n",
      "8.106832504272461\n",
      "8.587238311767578\n",
      "8.102036476135254\n",
      "8.077091217041016\n",
      "8.341732025146484\n",
      "7.7785444259643555\n",
      "8.706184387207031\n",
      "8.483250617980957\n",
      "8.044302940368652\n",
      "6.538173198699951\n",
      "8.914020538330078\n",
      "8.931318283081055\n",
      "7.37255859375\n",
      "11.59337043762207\n",
      "9.209380149841309\n",
      "8.21045970916748\n",
      "Validation\n",
      "11.387295722961426\n",
      "12.987465858459473\n",
      "13.476447105407715\n",
      "12.26002311706543\n",
      "11.197000503540039\n",
      "12.144821166992188\n",
      "9.398828506469727\n",
      "10.366090774536133\n",
      "10.747282028198242\n",
      "13.235671043395996\n",
      "10.120389938354492\n",
      "13.540382385253906\n",
      "12.173967361450195\n",
      "12.70798397064209\n",
      "11.850773811340332\n",
      "13.218780517578125\n",
      "12.843814849853516\n",
      "11.153732299804688\n",
      "11.649130821228027\n",
      "10.235209465026855\n",
      "10.760723114013672\n",
      "10.054251670837402\n",
      "11.624041557312012\n",
      "12.797161102294922\n",
      "10.651729583740234\n",
      "12.99673843383789\n",
      "12.899185180664062\n",
      "10.859651565551758\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def train(model, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss': [], 'validation_loss': []}  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        # model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            data['input_ids'], data['abs'], data['ext'] = data['input_ids'].to(device), data['abs'].to(device), data['ext'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data['input_ids'], data['abs'], data['ext'])\n",
    "            loss = output[0] + output[1]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.item())\n",
    "            print(loss.item())\n",
    "\n",
    "        print(\"Validation\")\n",
    "        model.eval()\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            with torch.no_grad():\n",
    "              data['input_ids'], data['abs'], data['ext'] = data['input_ids'].to(device), data['abs'].to(device), data['ext'].to(device)\n",
    "              output = model(data['input_ids'], data['abs'], data['ext'])\n",
    "              loss = output[0] + output[1]\n",
    "              useful_stuff['validation_loss'].append(loss.item())\n",
    "              print(loss.item())\n",
    "\n",
    "              predictions = myDecoderModel.generate(data['input_ids'], encoder_outputs=output[2])\n",
    "              labels = data['abs'].cpu()\n",
    "\n",
    "              decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "              # Replace -100 in the labels as we can't decode them.\n",
    "              labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "              decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "              \n",
    "              # Rouge expects a newline after each sentence\n",
    "              decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "              decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "              metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "              \n",
    "        result = metric.compute(use_stemmer=True)\n",
    "        # Extract a few results from ROUGE\n",
    "        result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "    \n",
    "    return (useful_stuff, result)\n",
    "\n",
    "training_results = train(model, train_loader, validation_loader, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2a1e396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f84b8130e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_results[0]['training_loss'], label=\"train\")\n",
    "plt.plot(training_results[0]['validation_loss'], label=\"val\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.legend()\n",
    "#plt.title('training loss iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94fdad87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d44fd335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 18 22:12:00 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      Off  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    41W / 250W |  39975MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-PCIE-40GB      Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    40W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-PCIE-40GB      Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    37W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-PCIE-40GB      Off  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    37W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-PCIE-40GB      Off  | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    41W / 250W |   9187MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     15344      C   python                           1971MiB |\n",
      "|    0   N/A  N/A   2002260      C   .../obj_detection/bin/python    36901MiB |\n",
      "|    0   N/A  N/A   2026001      C   python                            603MiB |\n",
      "|    0   N/A  N/A   2839719      C   python                            497MiB |\n",
      "|    1   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    2   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    3   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A     65424      C   ...onda3/envs/sb3/bin/python     8601MiB |\n",
      "|    4   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('allenai-led-base-16384.pickle', 'wb') as f:\n",
    "    pickle.dump(training_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033ec07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
