{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4a4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "import copy\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0e4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-large-xsum\"\n",
    "batch_size = 2\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d823f557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b6e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 17 11:49:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      Off  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    38W / 250W |   8104MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-PCIE-40GB      Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   67C    P0   247W / 250W |  40137MiB / 40536MiB |     34%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-PCIE-40GB      Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    37W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-PCIE-40GB      Off  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    37W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-PCIE-40GB      Off  | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    39W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    914655      C   .../obj_detection/bin/python     5979MiB |\n",
      "|    0   N/A  N/A   1876506      C   python                            499MiB |\n",
      "|    0   N/A  N/A   1877060      C   python                            523MiB |\n",
      "|    0   N/A  N/A   2026001      C   python                            603MiB |\n",
      "|    0   N/A  N/A   2839719      C   python                            497MiB |\n",
      "|    1   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    1   N/A  N/A   3404078      C   python                          39551MiB |\n",
      "|    2   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    3   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9147b647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a13d8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a072e4bbea4fbba7edd456e6733792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e709749e1b4343a0bd61594f400a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981c5dfc5f134fe084e51a438171356f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49aef3bbb2d4f45b5b4d60a8dc22850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df89d69bd344e0a9359f9f5a4393220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19155845df1045649ae212dab3b6db76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the function\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "# define the class\n",
    "class MLT(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_checkpoint):\n",
    "      super(MLT, self).__init__()\n",
    "\n",
    "      self.model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "      self.encoder = self.model.get_encoder()\n",
    "\n",
    "      self.decoder1 = self.model.get_decoder()\n",
    "      self.decoder2 = copy.deepcopy(self.decoder1)\n",
    "\n",
    "      self.lm_head1 = self.model.get_output_embeddings()\n",
    "      self.lm_head2 = copy.deepcopy(self.lm_head1)\n",
    "\n",
    "    def get_config(self):\n",
    "      return self.model.config\n",
    "\n",
    "    def get_decoder(self):\n",
    "      return self.decoder1\n",
    "\n",
    "    def get_lm_head(self):\n",
    "      return self.lm_head1\n",
    "\n",
    "    def get_final_logits_bias(self):\n",
    "      return self.model.final_logits_bias\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "      return self.tokenizer\n",
    "\n",
    "    def forward(self, text, summary1, summary2):\n",
    "      # inputs = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "      # target1 = self.tokenizer.encode(summary1, return_tensors=\"pt\")\n",
    "      # target2 = self.tokenizer.encode(summary2, return_tensors=\"pt\")\n",
    "\n",
    "      inputs = text\n",
    "      target1 = summary1\n",
    "      target2 = summary2\n",
    "\n",
    "      encoder_outputs = self.encoder(inputs)\n",
    "\n",
    "      decoder_input_ids1 = shift_tokens_right(\n",
    "                    target1, self.model.config.pad_token_id, self.model.config.decoder_start_token_id\n",
    "                )\n",
    "      \n",
    "      decoder_input_ids2 = shift_tokens_right(\n",
    "                    target2, self.model.config.pad_token_id, self.model.config.decoder_start_token_id\n",
    "                )\n",
    "      \n",
    "      \n",
    "      decoder_outputs1 = self.decoder1(\n",
    "          decoder_input_ids1, \n",
    "          encoder_hidden_states=encoder_outputs[0], \n",
    "          use_cache = False,\n",
    "          output_attentions=self.model.config.output_attentions,\n",
    "          output_hidden_states=self.model.config.output_hidden_states,\n",
    "          return_dict=self.model.config.use_return_dict,\n",
    "          ) \n",
    "\n",
    "      decoder_outputs2 = self.decoder2(\n",
    "          decoder_input_ids2, \n",
    "          encoder_hidden_states=encoder_outputs[0], \n",
    "          use_cache = False,\n",
    "          output_attentions=self.model.config.output_attentions,\n",
    "          output_hidden_states=self.model.config.output_hidden_states,\n",
    "          return_dict=self.model.config.use_return_dict,\n",
    "          )  \n",
    "\n",
    "      lm_logits1 = self.lm_head1(decoder_outputs1[0]) + self.model.final_logits_bias\n",
    "      lm_logits2 = self.lm_head2(decoder_outputs2[0]) + self.model.final_logits_bias   \n",
    "\n",
    "      masked_lm_loss1 = None\n",
    "      masked_lm_loss2 = None\n",
    "      loss_fct = CrossEntropyLoss()\n",
    "      masked_lm_loss1 = loss_fct(lm_logits1.view(-1, self.model.config.vocab_size), target1.view(-1))\n",
    "      masked_lm_loss2 = loss_fct(lm_logits2.view(-1, self.model.config.vocab_size), target2.view(-1))\n",
    "      \n",
    "      # return {\n",
    "      #     'loss1': masked_lm_loss1, \n",
    "      #     'loss2': masked_lm_loss2,\n",
    "      #     'encoder_outputs': encoder_outputs\n",
    "      #     }\n",
    "\n",
    "      return (masked_lm_loss1, masked_lm_loss2, encoder_outputs)\n",
    "\n",
    "\n",
    "# create the object\n",
    "model = MLT(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27590c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLT(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder1): BartDecoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder2): BartDecoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head1): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "  (lm_head2): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc85895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "137\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "transcripts_dir = Path(\"./data/ami/transcripts\")\n",
    "abs_summaries_dir = Path(\"./data/ami/summaries/abstractive\")\n",
    "ext_summaries_dir = Path(\"./data/ami/summaries/extractive\")\n",
    "\n",
    "transcripts = []\n",
    "abs_summaries = []\n",
    "ext_summaries = []\n",
    "\n",
    "for file in transcripts_dir.iterdir():\n",
    "  transcripts.append(file.read_text())\n",
    "\n",
    "for file in abs_summaries_dir.iterdir():\n",
    "  abs_summaries.append(file.read_text())\n",
    "\n",
    "for file in ext_summaries_dir.iterdir():\n",
    "  ext_summaries.append(file.read_text())\n",
    "\n",
    "print(len(transcripts))\n",
    "print(len(abs_summaries))\n",
    "print(len(ext_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58b2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_transcripts, val_transcripts, train_abs_summaries, val_abs_summaries = train_test_split(transcripts, abs_summaries, test_size=.2)\n",
    "_, _, train_ext_summaries, val_ext_summaries = train_test_split(transcripts, ext_summaries, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0016b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "train_transcripts_encodings = tokenizer(train_transcripts, truncation=True, padding=True)\n",
    "val_transcripts_encodings = tokenizer(val_transcripts, truncation=True, padding=True)\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "  train_abs_summaries_encodings = tokenizer(train_abs_summaries, truncation=True, padding=True)\n",
    "  val_abs_summaries_encodings = tokenizer(val_abs_summaries, truncation=True, padding=True)\n",
    "\n",
    "  train_ext_summaries_encodings = tokenizer(train_ext_summaries, truncation=True, padding=True)\n",
    "  val_ext_summaries_encodings = tokenizer(val_ext_summaries, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0e2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transcripts, abs_summaries, ext_summaries):\n",
    "        self.transcripts = transcripts\n",
    "        self.abs_summaries = abs_summaries\n",
    "        self.ext_summaries = ext_summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transcripts.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.transcripts.items()}\n",
    "        item[\"abs\"] = torch.tensor(self.abs_summaries[\"input_ids\"][idx])\n",
    "        item[\"ext\"] = torch.tensor(self.ext_summaries[\"input_ids\"][idx])\n",
    "        return item\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = MeetDataset(train_transcripts_encodings, train_abs_summaries_encodings, train_ext_summaries_encodings)\n",
    "val_dataset = MeetDataset(val_transcripts_encodings, val_abs_summaries_encodings, val_ext_summaries_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2175d04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__(), val_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f20ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f49d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartPretrainedModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class DecoderForGeneration(BartPretrainedModel):\n",
    "\n",
    "    def __init__(self, config, decoder, lm_head, final_logits_bias):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.config = config\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "        self.final_logits_bias = final_logits_bias\n",
    "\n",
    "    # def get_encoder(self):\n",
    "    #     return self.get_encoder()\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.get_decoder()\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
    "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
    "        self._resize_final_logits_bias(new_num_tokens)\n",
    "        return new_embeddings\n",
    "\n",
    "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
    "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
    "        if new_num_tokens <= old_num_tokens:\n",
    "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
    "        else:\n",
    "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
    "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
    "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            # if use_cache:\n",
    "            #     logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
    "            use_cache = False\n",
    "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "\n",
    "        decoder_outputs = self.decoder(\n",
    "        decoder_input_ids, \n",
    "        encoder_hidden_states=encoder_outputs[0], \n",
    "        use_cache = False,\n",
    "        output_attentions=self.config.output_attentions,\n",
    "        output_hidden_states=self.config.output_hidden_states,\n",
    "        return_dict=self.config.use_return_dict,\n",
    "        )\n",
    "        lm_logits = self.lm_head(decoder_outputs[0]) + self.final_logits_bias\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        decoder_input_ids,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"past_key_values\": past,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
    "        }\n",
    "\n",
    "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
    "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
    "            )\n",
    "        return reordered_past\n",
    "\n",
    "\n",
    "myDecoderModel = DecoderForGeneration(model.get_config(), model.get_decoder(), model.get_lm_head(), model.get_final_logits_bias())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec54ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderForGeneration(\n",
       "  (decoder): BartDecoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDecoderModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e5dd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4284777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2959250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/tanik_1821cs08/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "16.444766998291016\n",
      "14.254425048828125\n",
      "13.046048164367676\n",
      "8.702168464660645\n",
      "6.0632429122924805\n",
      "5.22963285446167\n",
      "5.329306602478027\n",
      "5.693272113800049\n",
      "6.7725019454956055\n",
      "5.903493881225586\n",
      "5.687968730926514\n",
      "6.927517890930176\n",
      "5.206056118011475\n",
      "5.3506574630737305\n",
      "5.6425933837890625\n",
      "5.54994010925293\n",
      "5.096153736114502\n",
      "4.83039665222168\n",
      "4.994567394256592\n",
      "4.9212188720703125\n",
      "5.415584564208984\n",
      "5.137579917907715\n",
      "5.096724510192871\n",
      "5.004907608032227\n",
      "5.235454559326172\n",
      "5.208622932434082\n",
      "4.947494983673096\n",
      "4.831175804138184\n",
      "5.144267559051514\n",
      "4.965187072753906\n",
      "4.659221649169922\n",
      "5.07543420791626\n",
      "4.304231643676758\n",
      "4.986797332763672\n",
      "4.650906562805176\n",
      "4.470065116882324\n",
      "4.924275875091553\n",
      "4.670719623565674\n",
      "5.123287677764893\n",
      "4.42459774017334\n",
      "5.070436477661133\n",
      "4.824466705322266\n",
      "6.196460723876953\n",
      "4.877511978149414\n",
      "3.8349013328552246\n",
      "5.012655735015869\n",
      "4.458744525909424\n",
      "4.04240083694458\n",
      "5.233706474304199\n",
      "4.853124141693115\n",
      "4.845785140991211\n",
      "4.191359519958496\n",
      "5.512332439422607\n",
      "4.816342830657959\n",
      "4.010721683502197\n",
      "Validation\n",
      "6.579835891723633\n",
      "6.198940277099609\n",
      "6.608834266662598\n",
      "6.760028839111328\n",
      "5.9384307861328125\n",
      "6.2613725662231445\n",
      "5.518180847167969\n",
      "5.499471664428711\n",
      "3.936722755432129\n",
      "6.66603946685791\n",
      "6.511652946472168\n",
      "5.807238578796387\n",
      "6.291706085205078\n",
      "6.3418869972229\n",
      "1\n",
      "4.525629997253418\n",
      "4.524316310882568\n",
      "4.6980299949646\n",
      "4.739665985107422\n",
      "4.696239471435547\n",
      "4.678642749786377\n",
      "5.077759265899658\n",
      "4.517335891723633\n",
      "4.5640435218811035\n",
      "5.021269798278809\n",
      "4.886273384094238\n",
      "3.9039931297302246\n",
      "3.856706380844116\n",
      "4.4825358390808105\n",
      "4.135662078857422\n",
      "4.9993181228637695\n",
      "4.709897994995117\n",
      "3.558032512664795\n",
      "4.544475555419922\n",
      "4.615392684936523\n",
      "4.483328342437744\n",
      "4.374128818511963\n",
      "4.255331993103027\n",
      "4.476082801818848\n",
      "4.57645845413208\n",
      "5.411785125732422\n",
      "4.7117600440979\n",
      "4.917862415313721\n",
      "4.837133884429932\n",
      "4.670742511749268\n",
      "4.59494686126709\n",
      "4.664399147033691\n",
      "4.564590930938721\n",
      "4.497396945953369\n",
      "4.331302642822266\n",
      "4.489830493927002\n",
      "4.7323174476623535\n",
      "4.6234917640686035\n",
      "4.486757278442383\n",
      "4.612850189208984\n",
      "3.9698445796966553\n",
      "4.944592475891113\n",
      "4.098024845123291\n",
      "3.3003861904144287\n",
      "4.3384857177734375\n",
      "4.822948932647705\n",
      "4.029266834259033\n",
      "4.745009422302246\n",
      "4.939372539520264\n",
      "4.721271991729736\n",
      "4.774291515350342\n",
      "4.943354606628418\n",
      "3.804675579071045\n",
      "4.851825714111328\n",
      "3.933540105819702\n",
      "Validation\n",
      "6.227074146270752\n",
      "5.899477005004883\n",
      "6.2597761154174805\n",
      "6.477726459503174\n",
      "5.561208724975586\n",
      "6.019586563110352\n",
      "5.3131256103515625\n",
      "5.238670825958252\n",
      "3.705883502960205\n",
      "6.324582099914551\n",
      "6.189349174499512\n",
      "5.510405540466309\n",
      "5.952944755554199\n",
      "5.998534202575684\n",
      "2\n",
      "4.646249771118164\n",
      "4.60090446472168\n",
      "4.390952110290527\n",
      "4.734426021575928\n",
      "4.373584270477295\n",
      "4.0931901931762695\n",
      "4.078632354736328\n",
      "4.883180618286133\n",
      "3.8434324264526367\n",
      "4.452284812927246\n",
      "4.579793930053711\n",
      "4.245953559875488\n",
      "3.5311291217803955\n",
      "4.838677406311035\n",
      "4.574416160583496\n",
      "4.005143165588379\n",
      "4.33542537689209\n",
      "4.022953510284424\n",
      "4.649612903594971\n",
      "4.433304309844971\n",
      "4.7195634841918945\n",
      "4.459799289703369\n",
      "3.8042407035827637\n",
      "3.3537139892578125\n",
      "3.595198154449463\n",
      "5.793210029602051\n",
      "4.441349983215332\n",
      "4.785778045654297\n",
      "4.689155101776123\n",
      "4.40384578704834\n",
      "4.408956527709961\n",
      "4.123709201812744\n",
      "4.567402362823486\n",
      "4.269425392150879\n",
      "4.4230852127075195\n",
      "4.861565589904785\n",
      "3.163987398147583\n",
      "4.204923152923584\n",
      "4.367931842803955\n",
      "4.458776473999023\n",
      "4.303700923919678\n",
      "4.357614517211914\n",
      "3.7553634643554688\n",
      "4.092977046966553\n",
      "4.570570945739746\n",
      "3.1126697063446045\n",
      "4.819118499755859\n",
      "4.210012435913086\n",
      "4.518906116485596\n",
      "4.126546382904053\n",
      "4.588316917419434\n",
      "4.306515693664551\n",
      "4.161374092102051\n",
      "4.510501861572266\n",
      "4.048886299133301\n",
      "Validation\n",
      "6.106932640075684\n",
      "5.818424224853516\n",
      "6.170843124389648\n",
      "6.313938140869141\n",
      "5.3926615715026855\n",
      "5.864401817321777\n",
      "5.176955223083496\n",
      "5.083339214324951\n",
      "3.5766520500183105\n",
      "6.13698148727417\n",
      "6.093429088592529\n",
      "5.410137176513672\n",
      "5.871341228485107\n",
      "5.878093719482422\n",
      "3\n",
      "3.9498424530029297\n",
      "3.3141331672668457\n",
      "4.118041038513184\n",
      "4.190113067626953\n",
      "4.730059623718262\n",
      "4.071719646453857\n",
      "4.212473392486572\n",
      "4.490631103515625\n",
      "4.456881046295166\n",
      "4.102795124053955\n",
      "4.464133262634277\n",
      "4.487445831298828\n",
      "3.8813977241516113\n",
      "4.160360336303711\n",
      "4.365094184875488\n",
      "4.321859836578369\n",
      "3.949883460998535\n",
      "4.060040473937988\n",
      "4.393401145935059\n",
      "4.698446273803711\n",
      "3.981142520904541\n",
      "3.9220399856567383\n",
      "4.470584869384766\n",
      "4.275106430053711\n",
      "4.272182941436768\n",
      "4.456205368041992\n",
      "3.7903425693511963\n",
      "3.6015405654907227\n",
      "4.407048225402832\n",
      "5.895207405090332\n",
      "4.206292629241943\n",
      "4.27734375\n",
      "3.5224177837371826\n",
      "4.261415958404541\n",
      "4.459996223449707\n",
      "4.276447296142578\n",
      "4.638129711151123\n",
      "4.141314506530762\n",
      "4.39216947555542\n",
      "3.9966068267822266\n",
      "2.730522394180298\n",
      "4.556833744049072\n",
      "4.615468978881836\n",
      "4.164691925048828\n",
      "3.3466567993164062\n",
      "4.387182712554932\n",
      "4.2803544998168945\n",
      "4.379423141479492\n",
      "4.449355125427246\n",
      "4.277847766876221\n",
      "4.06642484664917\n",
      "4.301247596740723\n",
      "4.536323547363281\n",
      "4.647324085235596\n",
      "4.248158931732178\n",
      "Validation\n",
      "6.032451629638672\n",
      "5.753658294677734\n",
      "6.083388328552246\n",
      "6.240588188171387\n",
      "5.312836170196533\n",
      "5.784604549407959\n",
      "5.110177040100098\n",
      "5.036158561706543\n",
      "3.5302138328552246\n",
      "6.038837432861328\n",
      "6.020635604858398\n",
      "5.353418350219727\n",
      "5.7973175048828125\n",
      "5.803763389587402\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def train(model, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss': [], 'validation_loss': []}  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        # model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            data['input_ids'], data['abs'], data['ext'] = data['input_ids'].to(device), data['abs'].to(device), data['ext'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data['input_ids'], data['abs'], data['ext'])\n",
    "            loss = output[0] + output[1]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.item())\n",
    "            print(loss.item())\n",
    "\n",
    "        print(\"Validation\")\n",
    "        model.eval()\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            with torch.no_grad():\n",
    "              data['input_ids'], data['abs'], data['ext'] = data['input_ids'].to(device), data['abs'].to(device), data['ext'].to(device)\n",
    "              output = model(data['input_ids'], data['abs'], data['ext'])\n",
    "              loss = output[0] + output[1]\n",
    "              useful_stuff['validation_loss'].append(loss.item())\n",
    "              print(loss.item())\n",
    "\n",
    "              predictions = myDecoderModel.generate(data['input_ids'], encoder_outputs=output[2])\n",
    "              labels = data['abs'].cpu()\n",
    "\n",
    "              decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "              # Replace -100 in the labels as we can't decode them.\n",
    "              labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "              decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "              \n",
    "              # Rouge expects a newline after each sentence\n",
    "              decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "              decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "              metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "              \n",
    "        result = metric.compute(use_stemmer=True)\n",
    "        # Extract a few results from ROUGE\n",
    "        result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "    \n",
    "    return (useful_stuff, result)\n",
    "\n",
    "training_results = train(model, train_loader, validation_loader, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2a1e396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0c380146a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_results[0]['training_loss'], label=\"train\")\n",
    "plt.plot(training_results[0]['validation_loss'], label=\"val\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.legend()\n",
    "#plt.title('training loss iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94fdad87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 24.8916, 'rouge2': 9.4358, 'rougeL': 18.3784, 'rougeLsum': 23.868}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d44fd335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 17 11:54:33 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      Off  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    39W / 250W |   8208MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-PCIE-40GB      Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   66C    P0   241W / 250W |  40137MiB / 40536MiB |     96%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-PCIE-40GB      Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    36W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-PCIE-40GB      Off  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    36W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-PCIE-40GB      Off  | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   58C    P0    81W / 250W |  28105MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    914655      C   .../obj_detection/bin/python     5979MiB |\n",
      "|    0   N/A  N/A   1876506      C   python                            499MiB |\n",
      "|    0   N/A  N/A   1877060      C   python                            627MiB |\n",
      "|    0   N/A  N/A   2026001      C   python                            603MiB |\n",
      "|    0   N/A  N/A   2839719      C   python                            497MiB |\n",
      "|    1   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    1   N/A  N/A   3404078      C   python                          39551MiB |\n",
      "|    2   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    3   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A   1877982      C   ...onda3/envs/sb3/bin/python    27519MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6948a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('facebook-bart-large-xsum.pickle', 'wb') as f:\n",
    "    pickle.dump(training_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46700e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
