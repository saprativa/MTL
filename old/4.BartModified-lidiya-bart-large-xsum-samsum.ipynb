{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4a4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "import copy\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0e4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"lidiya/bart-large-xsum-samsum\"\n",
    "batch_size = 2\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d823f557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b6e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 17 18:09:01 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      Off  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    41W / 250W |  39002MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-PCIE-40GB      Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    40W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-PCIE-40GB      Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    36W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-PCIE-40GB      Off  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    36W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-PCIE-40GB      Off  | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   69C    P0   262W / 250W |  16269MiB / 40536MiB |     93%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1876506      C   python                            499MiB |\n",
      "|    0   N/A  N/A   1954462      C   python                            499MiB |\n",
      "|    0   N/A  N/A   2002260      C   .../obj_detection/bin/python    36901MiB |\n",
      "|    0   N/A  N/A   2026001      C   python                            603MiB |\n",
      "|    0   N/A  N/A   2839719      C   python                            497MiB |\n",
      "|    1   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    2   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    3   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A   2022249      C   python                          15683MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9147b647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a13d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "# define the class\n",
    "class MLT(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_checkpoint):\n",
    "      super(MLT, self).__init__()\n",
    "\n",
    "      self.model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "      self.encoder = self.model.get_encoder()\n",
    "\n",
    "      self.decoder1 = self.model.get_decoder()\n",
    "      self.decoder2 = copy.deepcopy(self.decoder1)\n",
    "\n",
    "      self.lm_head1 = self.model.get_output_embeddings()\n",
    "      self.lm_head2 = copy.deepcopy(self.lm_head1)\n",
    "\n",
    "    def get_config(self):\n",
    "      return self.model.config\n",
    "\n",
    "    def get_decoder(self):\n",
    "      return self.decoder1\n",
    "\n",
    "    def get_lm_head(self):\n",
    "      return self.lm_head1\n",
    "\n",
    "    def get_final_logits_bias(self):\n",
    "      return self.model.final_logits_bias\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "      return self.tokenizer\n",
    "\n",
    "    def forward(self, text, summary1, summary2):\n",
    "      # inputs = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "      # target1 = self.tokenizer.encode(summary1, return_tensors=\"pt\")\n",
    "      # target2 = self.tokenizer.encode(summary2, return_tensors=\"pt\")\n",
    "\n",
    "      inputs = text\n",
    "      target1 = summary1\n",
    "      target2 = summary2\n",
    "\n",
    "      encoder_outputs = self.encoder(inputs)\n",
    "\n",
    "      decoder_input_ids1 = shift_tokens_right(\n",
    "                    target1, self.model.config.pad_token_id, self.model.config.decoder_start_token_id\n",
    "                )\n",
    "      \n",
    "      decoder_input_ids2 = shift_tokens_right(\n",
    "                    target2, self.model.config.pad_token_id, self.model.config.decoder_start_token_id\n",
    "                )\n",
    "      \n",
    "      \n",
    "      decoder_outputs1 = self.decoder1(\n",
    "          decoder_input_ids1, \n",
    "          encoder_hidden_states=encoder_outputs[0], \n",
    "          use_cache = False,\n",
    "          output_attentions=self.model.config.output_attentions,\n",
    "          output_hidden_states=self.model.config.output_hidden_states,\n",
    "          return_dict=self.model.config.use_return_dict,\n",
    "          ) \n",
    "\n",
    "      decoder_outputs2 = self.decoder2(\n",
    "          decoder_input_ids2, \n",
    "          encoder_hidden_states=encoder_outputs[0], \n",
    "          use_cache = False,\n",
    "          output_attentions=self.model.config.output_attentions,\n",
    "          output_hidden_states=self.model.config.output_hidden_states,\n",
    "          return_dict=self.model.config.use_return_dict,\n",
    "          )  \n",
    "\n",
    "      lm_logits1 = self.lm_head1(decoder_outputs1[0]) + self.model.final_logits_bias\n",
    "      lm_logits2 = self.lm_head2(decoder_outputs2[0]) + self.model.final_logits_bias   \n",
    "\n",
    "      masked_lm_loss1 = None\n",
    "      masked_lm_loss2 = None\n",
    "      loss_fct = CrossEntropyLoss()\n",
    "      masked_lm_loss1 = loss_fct(lm_logits1.view(-1, self.model.config.vocab_size), target1.view(-1))\n",
    "      masked_lm_loss2 = loss_fct(lm_logits2.view(-1, self.model.config.vocab_size), target2.view(-1))\n",
    "      \n",
    "      # return {\n",
    "      #     'loss1': masked_lm_loss1, \n",
    "      #     'loss2': masked_lm_loss2,\n",
    "      #     'encoder_outputs': encoder_outputs\n",
    "      #     }\n",
    "\n",
    "      return (masked_lm_loss1, masked_lm_loss2, encoder_outputs)\n",
    "\n",
    "\n",
    "# create the object\n",
    "model = MLT(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27590c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLT(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder1): BartDecoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder2): BartDecoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head1): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "  (lm_head2): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc85895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "137\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "transcripts_dir = Path(\"./data/ami/transcripts\")\n",
    "abs_summaries_dir = Path(\"./data/ami/summaries/abstractive\")\n",
    "ext_summaries_dir = Path(\"./data/ami/summaries/extractive\")\n",
    "\n",
    "transcripts = []\n",
    "abs_summaries = []\n",
    "ext_summaries = []\n",
    "\n",
    "for file in transcripts_dir.iterdir():\n",
    "  transcripts.append(file.read_text())\n",
    "\n",
    "for file in abs_summaries_dir.iterdir():\n",
    "  abs_summaries.append(file.read_text())\n",
    "\n",
    "for file in ext_summaries_dir.iterdir():\n",
    "  ext_summaries.append(file.read_text())\n",
    "\n",
    "print(len(transcripts))\n",
    "print(len(abs_summaries))\n",
    "print(len(ext_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58b2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_transcripts, val_transcripts, train_abs_summaries, val_abs_summaries = train_test_split(transcripts, abs_summaries, test_size=.2)\n",
    "_, _, train_ext_summaries, val_ext_summaries = train_test_split(transcripts, ext_summaries, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0016b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "train_transcripts_encodings = tokenizer(train_transcripts, truncation=True, padding=True)\n",
    "val_transcripts_encodings = tokenizer(val_transcripts, truncation=True, padding=True)\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "  train_abs_summaries_encodings = tokenizer(train_abs_summaries, truncation=True, padding=True)\n",
    "  val_abs_summaries_encodings = tokenizer(val_abs_summaries, truncation=True, padding=True)\n",
    "\n",
    "  train_ext_summaries_encodings = tokenizer(train_ext_summaries, truncation=True, padding=True)\n",
    "  val_ext_summaries_encodings = tokenizer(val_ext_summaries, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0e2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transcripts, abs_summaries, ext_summaries):\n",
    "        self.transcripts = transcripts\n",
    "        self.abs_summaries = abs_summaries\n",
    "        self.ext_summaries = ext_summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transcripts.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.transcripts.items()}\n",
    "        item[\"abs\"] = torch.tensor(self.abs_summaries[\"input_ids\"][idx])\n",
    "        item[\"ext\"] = torch.tensor(self.ext_summaries[\"input_ids\"][idx])\n",
    "        return item\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = MeetDataset(train_transcripts_encodings, train_abs_summaries_encodings, train_ext_summaries_encodings)\n",
    "val_dataset = MeetDataset(val_transcripts_encodings, val_abs_summaries_encodings, val_ext_summaries_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2175d04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__(), val_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f20ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f49d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartPretrainedModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class DecoderForGeneration(BartPretrainedModel):\n",
    "\n",
    "    def __init__(self, config, decoder, lm_head, final_logits_bias):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.config = config\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "        self.final_logits_bias = final_logits_bias\n",
    "\n",
    "    # def get_encoder(self):\n",
    "    #     return self.get_encoder()\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.get_decoder()\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
    "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
    "        self._resize_final_logits_bias(new_num_tokens)\n",
    "        return new_embeddings\n",
    "\n",
    "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
    "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
    "        if new_num_tokens <= old_num_tokens:\n",
    "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
    "        else:\n",
    "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
    "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
    "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            # if use_cache:\n",
    "            #     logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
    "            use_cache = False\n",
    "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "\n",
    "        decoder_outputs = self.decoder(\n",
    "        decoder_input_ids, \n",
    "        encoder_hidden_states=encoder_outputs[0], \n",
    "        use_cache = False,\n",
    "        output_attentions=self.config.output_attentions,\n",
    "        output_hidden_states=self.config.output_hidden_states,\n",
    "        return_dict=self.config.use_return_dict,\n",
    "        )\n",
    "        lm_logits = self.lm_head(decoder_outputs[0]) + self.final_logits_bias\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        decoder_input_ids,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"past_key_values\": past,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
    "        }\n",
    "\n",
    "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
    "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
    "            )\n",
    "        return reordered_past\n",
    "\n",
    "\n",
    "myDecoderModel = DecoderForGeneration(model.get_config(), model.get_decoder(), model.get_lm_head(), model.get_final_logits_bias())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec54ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderForGeneration(\n",
       "  (decoder): BartDecoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDecoderModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e5dd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4284777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2959250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/tanik_1821cs08/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "23.514739990234375\n",
      "18.731096267700195\n",
      "17.975008010864258\n",
      "14.933486938476562\n",
      "19.568492889404297\n",
      "11.93701171875\n",
      "19.600318908691406\n",
      "12.605157852172852\n",
      "8.10746955871582\n",
      "8.597329139709473\n",
      "7.712398529052734\n",
      "15.933422088623047\n",
      "15.370186805725098\n",
      "24.955644607543945\n",
      "11.929849624633789\n",
      "16.675405502319336\n",
      "44.48268127441406\n",
      "18.233192443847656\n",
      "12.593582153320312\n",
      "25.838590621948242\n",
      "16.58999252319336\n",
      "11.085458755493164\n",
      "13.532731056213379\n",
      "11.635902404785156\n",
      "10.090259552001953\n",
      "9.989038467407227\n",
      "11.043449401855469\n",
      "11.159810066223145\n",
      "9.906757354736328\n",
      "11.141210556030273\n",
      "8.94377326965332\n",
      "12.941177368164062\n",
      "12.507015228271484\n",
      "8.451904296875\n",
      "9.025440216064453\n",
      "8.85655689239502\n",
      "9.541431427001953\n",
      "9.435931205749512\n",
      "9.23583984375\n",
      "9.563977241516113\n",
      "9.36412239074707\n",
      "9.734914779663086\n",
      "8.79946517944336\n",
      "9.540997505187988\n",
      "9.246604919433594\n",
      "9.311582565307617\n",
      "8.833672523498535\n",
      "9.060776710510254\n",
      "8.771188735961914\n",
      "8.54783821105957\n",
      "8.974563598632812\n",
      "9.168396949768066\n",
      "9.70892333984375\n",
      "9.144271850585938\n",
      "9.061704635620117\n",
      "Validation\n",
      "10.660823822021484\n",
      "10.46706771850586\n",
      "11.292293548583984\n",
      "10.859123229980469\n",
      "10.755176544189453\n",
      "11.966768264770508\n",
      "11.983065605163574\n",
      "12.102622985839844\n",
      "12.106569290161133\n",
      "11.195825576782227\n",
      "13.319145202636719\n",
      "11.005064964294434\n",
      "9.971595764160156\n",
      "11.811509132385254\n",
      "1\n",
      "9.315022468566895\n",
      "9.365951538085938\n",
      "9.697872161865234\n",
      "9.221702575683594\n",
      "8.56695556640625\n",
      "9.854232788085938\n",
      "9.252340316772461\n",
      "9.538342475891113\n",
      "8.832411766052246\n",
      "8.319183349609375\n",
      "11.391714096069336\n",
      "10.625659942626953\n",
      "9.961591720581055\n",
      "9.364217758178711\n",
      "7.724633693695068\n",
      "9.42490005493164\n",
      "8.583799362182617\n",
      "8.431524276733398\n",
      "7.8995137214660645\n",
      "9.425747871398926\n",
      "9.403481483459473\n",
      "8.638792991638184\n",
      "8.470739364624023\n",
      "8.995782852172852\n",
      "9.30495548248291\n",
      "8.351001739501953\n",
      "7.623068809509277\n",
      "8.998908042907715\n",
      "8.897721290588379\n",
      "8.209161758422852\n",
      "9.668351173400879\n",
      "8.66833782196045\n",
      "8.699246406555176\n",
      "9.131332397460938\n",
      "8.6700439453125\n",
      "8.81615924835205\n",
      "8.178654670715332\n",
      "8.345380783081055\n",
      "8.533084869384766\n",
      "8.520133018493652\n",
      "8.41044807434082\n",
      "8.53663158416748\n",
      "9.238042831420898\n",
      "9.500716209411621\n",
      "8.380498886108398\n",
      "8.889409065246582\n",
      "8.643433570861816\n",
      "8.35202407836914\n",
      "9.243818283081055\n",
      "8.635401725769043\n",
      "9.6656494140625\n",
      "9.174182891845703\n",
      "9.339591979980469\n",
      "8.55040454864502\n",
      "9.316716194152832\n",
      "Validation\n",
      "10.473163604736328\n",
      "9.826801300048828\n",
      "10.8178129196167\n",
      "10.674942016601562\n",
      "10.6375150680542\n",
      "11.752655029296875\n",
      "11.703096389770508\n",
      "11.861611366271973\n",
      "11.839323043823242\n",
      "10.775362014770508\n",
      "12.925660133361816\n",
      "10.809070587158203\n",
      "9.800444602966309\n",
      "11.570920944213867\n",
      "2\n",
      "8.966693878173828\n",
      "9.004734992980957\n",
      "9.185824394226074\n",
      "8.81606674194336\n",
      "7.760500907897949\n",
      "9.121965408325195\n",
      "8.957924842834473\n",
      "8.81328010559082\n",
      "7.50482177734375\n",
      "8.138138771057129\n",
      "8.671669960021973\n",
      "9.488920211791992\n",
      "8.68169116973877\n",
      "9.803531646728516\n",
      "8.488054275512695\n",
      "9.815116882324219\n",
      "8.874960899353027\n",
      "8.195992469787598\n",
      "9.034774780273438\n",
      "8.44018268585205\n",
      "8.766324996948242\n",
      "8.161385536193848\n",
      "8.638246536254883\n",
      "8.698952674865723\n",
      "7.647911071777344\n",
      "8.894899368286133\n",
      "8.719196319580078\n",
      "8.240615844726562\n",
      "8.17194652557373\n",
      "9.375838279724121\n",
      "7.9576568603515625\n",
      "8.537715911865234\n",
      "8.487202644348145\n",
      "8.53437614440918\n",
      "8.081989288330078\n",
      "8.947945594787598\n",
      "8.629596710205078\n",
      "9.125633239746094\n",
      "9.043780326843262\n",
      "9.389687538146973\n",
      "9.20882797241211\n",
      "9.00072956085205\n",
      "9.078301429748535\n",
      "11.659296035766602\n",
      "10.519277572631836\n",
      "8.727221488952637\n",
      "8.949992179870605\n",
      "8.340011596679688\n",
      "8.905689239501953\n",
      "8.463045120239258\n",
      "7.871767044067383\n",
      "8.67265796661377\n",
      "8.020843505859375\n",
      "8.207756996154785\n",
      "8.894790649414062\n",
      "Validation\n",
      "10.340034484863281\n",
      "9.676836013793945\n",
      "10.74363899230957\n",
      "10.581493377685547\n",
      "10.555938720703125\n",
      "11.752528190612793\n",
      "11.6348876953125\n",
      "11.828482627868652\n",
      "11.843381881713867\n",
      "10.678077697753906\n",
      "12.978294372558594\n",
      "10.787264823913574\n",
      "9.642382621765137\n",
      "11.523994445800781\n",
      "3\n",
      "8.823444366455078\n",
      "9.564970970153809\n",
      "8.55137825012207\n",
      "9.087997436523438\n",
      "8.629083633422852\n",
      "9.155805587768555\n",
      "8.216314315795898\n",
      "8.658408164978027\n",
      "8.294816970825195\n",
      "7.469738960266113\n",
      "7.7573676109313965\n",
      "8.484077453613281\n",
      "8.418753623962402\n",
      "7.76951789855957\n",
      "8.345283508300781\n",
      "8.117695808410645\n",
      "8.57979965209961\n",
      "8.550256729125977\n",
      "9.088682174682617\n",
      "9.503456115722656\n",
      "8.958646774291992\n",
      "8.17957878112793\n",
      "8.836551666259766\n",
      "7.961030006408691\n",
      "9.504508018493652\n",
      "8.753676414489746\n",
      "8.86350154876709\n",
      "8.938644409179688\n",
      "9.088233947753906\n",
      "8.735382080078125\n",
      "9.327825546264648\n",
      "9.181014060974121\n",
      "11.843236923217773\n",
      "10.150041580200195\n",
      "8.832498550415039\n",
      "9.018899917602539\n",
      "7.9534149169921875\n",
      "8.268749237060547\n",
      "8.283596992492676\n",
      "8.278371810913086\n",
      "8.10731315612793\n",
      "8.789981842041016\n",
      "8.329459190368652\n",
      "8.726448059082031\n",
      "8.791692733764648\n",
      "8.637462615966797\n",
      "8.004096984863281\n",
      "8.652814865112305\n",
      "8.214517593383789\n",
      "7.639320373535156\n",
      "9.025054931640625\n",
      "8.974959373474121\n",
      "8.170842170715332\n",
      "8.342253684997559\n",
      "8.841741561889648\n",
      "Validation\n",
      "10.294973373413086\n",
      "9.536678314208984\n",
      "10.62995719909668\n",
      "10.511913299560547\n",
      "10.490629196166992\n",
      "11.651758193969727\n",
      "11.534042358398438\n",
      "11.72042465209961\n",
      "11.758016586303711\n",
      "10.55536937713623\n",
      "12.826391220092773\n",
      "10.692441940307617\n",
      "9.591299057006836\n",
      "11.413818359375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def train(model, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss': [], 'validation_loss': []}  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        # model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            data['input_ids'], data['abs'], data['ext'] = data['input_ids'].to(device), data['abs'].to(device), data['ext'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data['input_ids'], data['abs'], data['ext'])\n",
    "            loss = output[0] + output[1]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.item())\n",
    "            print(loss.item())\n",
    "\n",
    "        print(\"Validation\")\n",
    "        model.eval()\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            with torch.no_grad():\n",
    "              data['input_ids'], data['abs'], data['ext'] = data['input_ids'].to(device), data['abs'].to(device), data['ext'].to(device)\n",
    "              output = model(data['input_ids'], data['abs'], data['ext'])\n",
    "              loss = output[0] + output[1]\n",
    "              useful_stuff['validation_loss'].append(loss.item())\n",
    "              print(loss.item())\n",
    "\n",
    "              predictions = myDecoderModel.generate(data['input_ids'], encoder_outputs=output[2])\n",
    "              labels = data['abs'].cpu()\n",
    "\n",
    "              decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "              # Replace -100 in the labels as we can't decode them.\n",
    "              labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "              decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "              \n",
    "              # Rouge expects a newline after each sentence\n",
    "              decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "              decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "              metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "              \n",
    "        result = metric.compute(use_stemmer=True)\n",
    "        # Extract a few results from ROUGE\n",
    "        result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "    \n",
    "    return (useful_stuff, result)\n",
    "\n",
    "training_results = train(model, train_loader, validation_loader, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2a1e396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7d4c7a7f10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_results[0]['training_loss'], label=\"train\")\n",
    "plt.plot(training_results[0]['validation_loss'], label=\"val\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.legend()\n",
    "#plt.title('training loss iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94fdad87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 17.3908, 'rouge2': 1.2064, 'rougeL': 11.9947, 'rougeLsum': 14.3612}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d44fd335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 17 18:12:19 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      Off  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    42W / 250W |  39002MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-PCIE-40GB      Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    40W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-PCIE-40GB      Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    36W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-PCIE-40GB      Off  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   59C    P0    76W / 250W |  28105MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-PCIE-40GB      Off  | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   69C    P0   251W / 250W |  16269MiB / 40536MiB |     96%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1876506      C   python                            499MiB |\n",
      "|    0   N/A  N/A   1954462      C   python                            499MiB |\n",
      "|    0   N/A  N/A   2002260      C   .../obj_detection/bin/python    36901MiB |\n",
      "|    0   N/A  N/A   2026001      C   python                            603MiB |\n",
      "|    0   N/A  N/A   2839719      C   python                            497MiB |\n",
      "|    1   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    2   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    3   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    3   N/A  N/A   2040070      C   ...onda3/envs/sb3/bin/python    27519MiB |\n",
      "|    4   N/A  N/A   2002260      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A   2022249      C   python                          15683MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6948a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lidiya-bart-large-xsum-samsum.pickle', 'wb') as f:\n",
    "    pickle.dump(training_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46700e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
