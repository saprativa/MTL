{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4a4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "import copy\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0e4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Salesforce/bart-large-xsum-samsum\"\n",
    "batch_size = 2\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d823f557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b6e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 17 11:55:58 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      Off  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    39W / 250W |   8282MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-PCIE-40GB      Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   69C    P0   265W / 250W |  40137MiB / 40536MiB |    100%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-PCIE-40GB      Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    36W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-PCIE-40GB      Off  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    37W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-PCIE-40GB      Off  | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    41W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    914655      C   .../obj_detection/bin/python     5979MiB |\n",
      "|    0   N/A  N/A   1876506      C   python                            499MiB |\n",
      "|    0   N/A  N/A   1877060      C   python                            701MiB |\n",
      "|    0   N/A  N/A   2026001      C   python                            603MiB |\n",
      "|    0   N/A  N/A   2839719      C   python                            497MiB |\n",
      "|    1   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    1   N/A  N/A   3404078      C   python                          39551MiB |\n",
      "|    2   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    3   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9147b647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a13d8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389f4cc9717f4e9fbc91172ff3f543ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8c42a00a0041e4bc2aaa5019278984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddca8f563b6d41df853e6271ddc07519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661cbd1ee1464a18a8c876ee4b301eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad7c5aa27d54c5886af68452866e68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8f285c66b644efa3f8afe0118d173a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the function\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "# define the class\n",
    "class MLT(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_checkpoint):\n",
    "      super(MLT, self).__init__()\n",
    "\n",
    "      self.model = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "      self.encoder = self.model.get_encoder()\n",
    "\n",
    "      self.decoder1 = self.model.get_decoder()\n",
    "      self.decoder2 = copy.deepcopy(self.decoder1)\n",
    "\n",
    "      self.lm_head1 = self.model.get_output_embeddings()\n",
    "      self.lm_head2 = copy.deepcopy(self.lm_head1)\n",
    "\n",
    "    def get_config(self):\n",
    "      return self.model.config\n",
    "\n",
    "    def get_decoder(self):\n",
    "      return self.decoder1\n",
    "\n",
    "    def get_lm_head(self):\n",
    "      return self.lm_head1\n",
    "\n",
    "    def get_final_logits_bias(self):\n",
    "      return self.model.final_logits_bias\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "      return self.tokenizer\n",
    "\n",
    "    def forward(self, text, summary1, summary2):\n",
    "      # inputs = self.tokenizer.encode(text, return_tensors=\"pt\")\n",
    "      # target1 = self.tokenizer.encode(summary1, return_tensors=\"pt\")\n",
    "      # target2 = self.tokenizer.encode(summary2, return_tensors=\"pt\")\n",
    "\n",
    "      inputs = text\n",
    "      target1 = summary1\n",
    "      target2 = summary2\n",
    "\n",
    "      encoder_outputs = self.encoder(inputs)\n",
    "\n",
    "      decoder_input_ids1 = shift_tokens_right(\n",
    "                    target1, self.model.config.pad_token_id, self.model.config.decoder_start_token_id\n",
    "                )\n",
    "      \n",
    "      decoder_input_ids2 = shift_tokens_right(\n",
    "                    target2, self.model.config.pad_token_id, self.model.config.decoder_start_token_id\n",
    "                )\n",
    "      \n",
    "      \n",
    "      decoder_outputs1 = self.decoder1(\n",
    "          decoder_input_ids1, \n",
    "          encoder_hidden_states=encoder_outputs[0], \n",
    "          use_cache = False,\n",
    "          output_attentions=self.model.config.output_attentions,\n",
    "          output_hidden_states=self.model.config.output_hidden_states,\n",
    "          return_dict=self.model.config.use_return_dict,\n",
    "          ) \n",
    "\n",
    "      decoder_outputs2 = self.decoder2(\n",
    "          decoder_input_ids2, \n",
    "          encoder_hidden_states=encoder_outputs[0], \n",
    "          use_cache = False,\n",
    "          output_attentions=self.model.config.output_attentions,\n",
    "          output_hidden_states=self.model.config.output_hidden_states,\n",
    "          return_dict=self.model.config.use_return_dict,\n",
    "          )  \n",
    "\n",
    "      lm_logits1 = self.lm_head1(decoder_outputs1[0]) + self.model.final_logits_bias\n",
    "      lm_logits2 = self.lm_head2(decoder_outputs2[0]) + self.model.final_logits_bias   \n",
    "\n",
    "      masked_lm_loss1 = None\n",
    "      masked_lm_loss2 = None\n",
    "      loss_fct = CrossEntropyLoss()\n",
    "      masked_lm_loss1 = loss_fct(lm_logits1.view(-1, self.model.config.vocab_size), target1.view(-1))\n",
    "      masked_lm_loss2 = loss_fct(lm_logits2.view(-1, self.model.config.vocab_size), target2.view(-1))\n",
    "      \n",
    "      # return {\n",
    "      #     'loss1': masked_lm_loss1, \n",
    "      #     'loss2': masked_lm_loss2,\n",
    "      #     'encoder_outputs': encoder_outputs\n",
    "      #     }\n",
    "\n",
    "      return (masked_lm_loss1, masked_lm_loss2, encoder_outputs)\n",
    "\n",
    "\n",
    "# create the object\n",
    "model = MLT(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27590c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLT(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "  )\n",
       "  (encoder): BartEncoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder1): BartDecoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder2): BartDecoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head1): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "  (lm_head2): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc85895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "137\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "transcripts_dir = Path(\"./data/ami/transcripts\")\n",
    "abs_summaries_dir = Path(\"./data/ami/summaries/abstractive\")\n",
    "ext_summaries_dir = Path(\"./data/ami/summaries/extractive\")\n",
    "\n",
    "transcripts = []\n",
    "abs_summaries = []\n",
    "ext_summaries = []\n",
    "\n",
    "for file in transcripts_dir.iterdir():\n",
    "  transcripts.append(file.read_text())\n",
    "\n",
    "for file in abs_summaries_dir.iterdir():\n",
    "  abs_summaries.append(file.read_text())\n",
    "\n",
    "for file in ext_summaries_dir.iterdir():\n",
    "  ext_summaries.append(file.read_text())\n",
    "\n",
    "print(len(transcripts))\n",
    "print(len(abs_summaries))\n",
    "print(len(ext_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58b2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_transcripts, val_transcripts, train_abs_summaries, val_abs_summaries = train_test_split(transcripts, abs_summaries, test_size=.2)\n",
    "_, _, train_ext_summaries, val_ext_summaries = train_test_split(transcripts, ext_summaries, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0016b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "train_transcripts_encodings = tokenizer(train_transcripts, truncation=True, padding=True)\n",
    "val_transcripts_encodings = tokenizer(val_transcripts, truncation=True, padding=True)\n",
    "\n",
    "with tokenizer.as_target_tokenizer():\n",
    "  train_abs_summaries_encodings = tokenizer(train_abs_summaries, truncation=True, padding=True)\n",
    "  val_abs_summaries_encodings = tokenizer(val_abs_summaries, truncation=True, padding=True)\n",
    "\n",
    "  train_ext_summaries_encodings = tokenizer(train_ext_summaries, truncation=True, padding=True)\n",
    "  val_ext_summaries_encodings = tokenizer(val_ext_summaries, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0e2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transcripts, abs_summaries, ext_summaries):\n",
    "        self.transcripts = transcripts\n",
    "        self.abs_summaries = abs_summaries\n",
    "        self.ext_summaries = ext_summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transcripts.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.transcripts.items()}\n",
    "        item[\"abs\"] = torch.tensor(self.abs_summaries[\"input_ids\"][idx])\n",
    "        item[\"ext\"] = torch.tensor(self.ext_summaries[\"input_ids\"][idx])\n",
    "        return item\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset = MeetDataset(train_transcripts_encodings, train_abs_summaries_encodings, train_ext_summaries_encodings)\n",
    "val_dataset = MeetDataset(val_transcripts_encodings, val_abs_summaries_encodings, val_ext_summaries_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2175d04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__(), val_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f20ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f49d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartPretrainedModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class DecoderForGeneration(BartPretrainedModel):\n",
    "\n",
    "    def __init__(self, config, decoder, lm_head, final_logits_bias):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.config = config\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "        self.final_logits_bias = final_logits_bias\n",
    "\n",
    "    # def get_encoder(self):\n",
    "    #     return self.get_encoder()\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model.get_decoder()\n",
    "\n",
    "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
    "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
    "        self._resize_final_logits_bias(new_num_tokens)\n",
    "        return new_embeddings\n",
    "\n",
    "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
    "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
    "        if new_num_tokens <= old_num_tokens:\n",
    "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
    "        else:\n",
    "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
    "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
    "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            # if use_cache:\n",
    "            #     logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
    "            use_cache = False\n",
    "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "\n",
    "        decoder_outputs = self.decoder(\n",
    "        decoder_input_ids, \n",
    "        encoder_hidden_states=encoder_outputs[0], \n",
    "        use_cache = False,\n",
    "        output_attentions=self.config.output_attentions,\n",
    "        output_hidden_states=self.config.output_hidden_states,\n",
    "        return_dict=self.config.use_return_dict,\n",
    "        )\n",
    "        lm_logits = self.lm_head(decoder_outputs[0]) + self.final_logits_bias\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + decoder_outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        decoder_input_ids,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"past_key_values\": past,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
    "        }\n",
    "\n",
    "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
    "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
    "            )\n",
    "        return reordered_past\n",
    "\n",
    "\n",
    "myDecoderModel = DecoderForGeneration(model.get_config(), model.get_decoder(), model.get_lm_head(), model.get_final_logits_bias())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec54ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderForGeneration(\n",
       "  (decoder): BartDecoder(\n",
       "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDecoderModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e5dd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4284777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2959250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/tanik_1821cs08/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20.407236099243164\n",
      "20.91741180419922\n",
      "19.326730728149414\n",
      "13.819416046142578\n",
      "15.221709251403809\n",
      "113.82301330566406\n",
      "42.41379165649414\n",
      "36.88201141357422\n",
      "23.679845809936523\n",
      "32.55747985839844\n",
      "22.87494468688965\n",
      "24.70960807800293\n",
      "20.782690048217773\n",
      "16.786361694335938\n",
      "20.710559844970703\n",
      "18.690048217773438\n",
      "20.832002639770508\n",
      "18.475448608398438\n",
      "20.586963653564453\n",
      "17.85483169555664\n",
      "17.8887939453125\n",
      "12.992992401123047\n",
      "13.194473266601562\n",
      "16.16101837158203\n",
      "13.92646598815918\n",
      "13.530242919921875\n",
      "14.297982215881348\n",
      "12.240488052368164\n",
      "11.271446228027344\n",
      "10.515932083129883\n",
      "11.679241180419922\n",
      "12.46891975402832\n",
      "29.043333053588867\n",
      "22.743114471435547\n",
      "28.89305877685547\n",
      "21.56205177307129\n",
      "25.87702178955078\n",
      "28.243263244628906\n",
      "22.933610916137695\n",
      "20.903541564941406\n",
      "22.183544158935547\n",
      "20.71725082397461\n",
      "26.7864990234375\n",
      "20.727561950683594\n",
      "20.701396942138672\n",
      "21.568225860595703\n",
      "16.752763748168945\n",
      "23.17405891418457\n",
      "20.962453842163086\n",
      "21.237504959106445\n",
      "20.861806869506836\n",
      "16.13625144958496\n",
      "34.065391540527344\n",
      "17.692737579345703\n",
      "23.51995849609375\n",
      "Validation\n",
      "30.76956558227539\n",
      "30.122699737548828\n",
      "26.33136749267578\n",
      "29.7900390625\n",
      "37.15296173095703\n",
      "27.719051361083984\n",
      "31.151790618896484\n",
      "24.492469787597656\n",
      "40.161338806152344\n",
      "34.477195739746094\n",
      "30.647830963134766\n",
      "28.950742721557617\n",
      "32.378074645996094\n",
      "33.9889030456543\n",
      "1\n",
      "19.43317413330078\n",
      "16.435054779052734\n",
      "16.6290340423584\n",
      "16.13790512084961\n",
      "17.087522506713867\n",
      "13.038500785827637\n",
      "14.61783504486084\n",
      "14.725860595703125\n",
      "15.754941940307617\n",
      "14.372429847717285\n",
      "12.914794921875\n",
      "13.170198440551758\n",
      "13.363343238830566\n",
      "13.057987213134766\n",
      "13.273458480834961\n",
      "11.503728866577148\n",
      "11.771783828735352\n",
      "10.599864959716797\n",
      "11.360940933227539\n",
      "15.93010139465332\n",
      "11.223644256591797\n",
      "10.71599006652832\n",
      "9.941777229309082\n",
      "9.274311065673828\n",
      "14.091987609863281\n",
      "20.773035049438477\n",
      "25.753650665283203\n",
      "18.802051544189453\n",
      "19.613788604736328\n",
      "22.827655792236328\n",
      "16.321239471435547\n",
      "20.905826568603516\n",
      "17.42509651184082\n",
      "21.68921661376953\n",
      "17.434476852416992\n",
      "16.516395568847656\n",
      "18.662857055664062\n",
      "15.691825866699219\n",
      "13.110907554626465\n",
      "17.226747512817383\n",
      "17.232437133789062\n",
      "14.436075210571289\n",
      "12.702832221984863\n",
      "15.085953712463379\n",
      "15.685340881347656\n",
      "12.76324462890625\n",
      "14.575748443603516\n",
      "12.973913192749023\n",
      "16.295011520385742\n",
      "16.7762393951416\n",
      "16.240747451782227\n",
      "15.603384017944336\n",
      "12.043200492858887\n",
      "14.877857208251953\n",
      "12.205954551696777\n",
      "Validation\n",
      "23.986698150634766\n",
      "23.798418045043945\n",
      "20.09662628173828\n",
      "22.361623764038086\n",
      "28.380268096923828\n",
      "21.541807174682617\n",
      "24.205888748168945\n",
      "19.87446403503418\n",
      "29.623443603515625\n",
      "25.86416244506836\n",
      "23.7934627532959\n",
      "22.993694305419922\n",
      "25.2055606842041\n",
      "24.991151809692383\n",
      "2\n",
      "13.898276329040527\n",
      "14.489532470703125\n",
      "11.73716926574707\n",
      "14.672748565673828\n",
      "13.477532386779785\n",
      "13.370800018310547\n",
      "13.87929916381836\n",
      "12.165655136108398\n",
      "11.744860649108887\n",
      "10.114073753356934\n",
      "17.098407745361328\n",
      "11.14150619506836\n",
      "10.23476791381836\n",
      "10.74978256225586\n",
      "10.152665138244629\n",
      "9.638023376464844\n",
      "9.659403800964355\n",
      "9.744654655456543\n",
      "9.176858901977539\n",
      "9.148533821105957\n",
      "9.399317741394043\n",
      "10.697591781616211\n",
      "17.905208587646484\n",
      "19.37311363220215\n",
      "14.134464263916016\n",
      "15.252670288085938\n",
      "15.346010208129883\n",
      "17.03594970703125\n",
      "16.713451385498047\n",
      "13.58570671081543\n",
      "15.818496704101562\n",
      "13.433584213256836\n",
      "16.179485321044922\n",
      "11.570706367492676\n",
      "14.876213073730469\n",
      "13.37000560760498\n",
      "14.627046585083008\n",
      "13.133590698242188\n",
      "14.109962463378906\n",
      "13.229776382446289\n",
      "14.259767532348633\n",
      "12.992171287536621\n",
      "14.117925643920898\n",
      "12.346075057983398\n",
      "12.510101318359375\n",
      "13.047706604003906\n",
      "10.648233413696289\n",
      "11.446647644042969\n",
      "12.652181625366211\n",
      "13.470354080200195\n",
      "11.980157852172852\n",
      "12.55143928527832\n",
      "12.569265365600586\n",
      "10.53730297088623\n",
      "11.350346565246582\n",
      "Validation\n",
      "17.813541412353516\n",
      "17.91095733642578\n",
      "14.994207382202148\n",
      "16.041736602783203\n",
      "20.517364501953125\n",
      "16.039648056030273\n",
      "17.843250274658203\n",
      "15.55908203125\n",
      "20.471920013427734\n",
      "18.362306594848633\n",
      "17.5521297454834\n",
      "17.488405227661133\n",
      "18.629592895507812\n",
      "17.402315139770508\n",
      "3\n",
      "8.551888465881348\n",
      "11.691149711608887\n",
      "11.249846458435059\n",
      "10.179530143737793\n",
      "10.733412742614746\n",
      "9.342620849609375\n",
      "10.224897384643555\n",
      "10.951711654663086\n",
      "9.442378997802734\n",
      "9.628036499023438\n",
      "9.869421005249023\n",
      "12.35015869140625\n",
      "9.416923522949219\n",
      "8.290157318115234\n",
      "9.309158325195312\n",
      "9.742273330688477\n",
      "9.591797828674316\n",
      "9.42891788482666\n",
      "9.097532272338867\n",
      "9.942647933959961\n",
      "11.668338775634766\n",
      "12.974830627441406\n",
      "12.520797729492188\n",
      "12.764297485351562\n",
      "12.15853500366211\n",
      "11.041555404663086\n",
      "10.880331993103027\n",
      "10.132680892944336\n",
      "11.53466796875\n",
      "12.616754531860352\n",
      "11.54759693145752\n",
      "11.56624984741211\n",
      "9.658880233764648\n",
      "9.832656860351562\n",
      "11.444351196289062\n",
      "10.919095993041992\n",
      "10.548635482788086\n",
      "10.136107444763184\n",
      "9.639530181884766\n",
      "8.95691967010498\n",
      "9.740041732788086\n",
      "9.952293395996094\n",
      "9.99024772644043\n",
      "9.445097923278809\n",
      "8.161247253417969\n",
      "8.659346580505371\n",
      "7.562865734100342\n",
      "9.14391803741455\n",
      "8.865571022033691\n",
      "9.855567932128906\n",
      "9.709882736206055\n",
      "8.86854362487793\n",
      "9.204163551330566\n",
      "9.11322021484375\n",
      "7.461634159088135\n",
      "Validation\n",
      "12.722694396972656\n",
      "13.304287910461426\n",
      "10.683418273925781\n",
      "10.470844268798828\n",
      "13.98668098449707\n",
      "11.512052536010742\n",
      "12.611680030822754\n",
      "12.418941497802734\n",
      "12.581713676452637\n",
      "11.908548355102539\n",
      "12.613285064697266\n",
      "13.234766006469727\n",
      "13.322190284729004\n",
      "10.655563354492188\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def train(model, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss': [], 'validation_loss': []}  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        # model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            data['input_ids'], data['abs'], data['ext'] = data['input_ids'].to(device), data['abs'].to(device), data['ext'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data['input_ids'], data['abs'], data['ext'])\n",
    "            loss = output[0] + output[1]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.item())\n",
    "            print(loss.item())\n",
    "\n",
    "        print(\"Validation\")\n",
    "        model.eval()\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            with torch.no_grad():\n",
    "              data['input_ids'], data['abs'], data['ext'] = data['input_ids'].to(device), data['abs'].to(device), data['ext'].to(device)\n",
    "              output = model(data['input_ids'], data['abs'], data['ext'])\n",
    "              loss = output[0] + output[1]\n",
    "              useful_stuff['validation_loss'].append(loss.item())\n",
    "              print(loss.item())\n",
    "\n",
    "              predictions = myDecoderModel.generate(data['input_ids'], encoder_outputs=output[2])\n",
    "              labels = data['abs'].cpu()\n",
    "\n",
    "              decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "              # Replace -100 in the labels as we can't decode them.\n",
    "              labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "              decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "              \n",
    "              # Rouge expects a newline after each sentence\n",
    "              decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "              decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "              metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "              \n",
    "        result = metric.compute(use_stemmer=True)\n",
    "        # Extract a few results from ROUGE\n",
    "        result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "    \n",
    "    return (useful_stuff, result)\n",
    "\n",
    "training_results = train(model, train_loader, validation_loader, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2a1e396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff359ff8a90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_results[0]['training_loss'], label=\"train\")\n",
    "plt.plot(training_results[0]['validation_loss'], label=\"val\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.legend()\n",
    "#plt.title('training loss iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94fdad87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 16.9723, 'rouge2': 1.0236, 'rougeL': 10.9669, 'rougeLsum': 14.3002}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d44fd335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 17 12:00:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      Off  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    38W / 250W |   7581MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-PCIE-40GB      Off  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   65C    P0   246W / 250W |  40137MiB / 40536MiB |    100%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-PCIE-40GB      Off  | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    36W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-PCIE-40GB      Off  | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    36W / 250W |    586MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-PCIE-40GB      Off  | 00000000:E1:00.0 Off |                    0 |\n",
      "| N/A   58C    P0    81W / 250W |  28105MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    914655      C   .../obj_detection/bin/python     5979MiB |\n",
      "|    0   N/A  N/A   1876506      C   python                            499MiB |\n",
      "|    0   N/A  N/A   2026001      C   python                            603MiB |\n",
      "|    0   N/A  N/A   2839719      C   python                            497MiB |\n",
      "|    1   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    1   N/A  N/A   3404078      C   python                          39551MiB |\n",
      "|    2   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    3   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A    914655      C   .../obj_detection/bin/python      583MiB |\n",
      "|    4   N/A  N/A   1880542      C   ...onda3/envs/sb3/bin/python    27519MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6948a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Salesforce-bart-large-xsum-samsum.pickle', 'wb') as f:\n",
    "    pickle.dump(training_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46700e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
